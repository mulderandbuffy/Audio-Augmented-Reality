@inproceedings{mynatt_designing_1998,
	address = {Los Angeles, California, United States},
	title = {Designing audio aura},
	isbn = {978-0-201-30987-4},
	url = {http://portal.acm.org/citation.cfm?doid=274644.274720},
	doi = {10.1145/274644.274720},
	abstract = {In this paper.we describethe processbehind the designof Audio Aura. The goal of Audio Aura is to provide serendipitousinformation, via backgroundauditory cues, thatis tied to people’sphysicalactionsin theworkplace.We used scenarios to explore issues in serendipitous information such as privacy and work practice. Our sound designwas guided by a number of strategiesfor creating peripheral sounds grouped in cohesive ecologies. Faced with an physical and software infrastructure under developmentin a laboratorydistantf?omour soundstudio. we prototypeddifferent soniclandscapesin VRML worlds. In our infrastructuredesign,we madeanumberof trade-offs in our useof legacysystemsandour client-serverdesign.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {Proceedings of the {SIGCHI} conference on {Human} factors in computing systems - {CHI} '98},
	publisher = {ACM Press},
	author = {Mynatt, Elizabeth D. and Back, Maribeth and Want, Roy and Baer, Michael and Ellis, Jason B.},
	year = {1998},
	pages = {566--573},
	file = {Mynatt et al. - 1998 - Designing audio aura.pdf:C\:\\Users\\tech\\Zotero\\storage\\67C5RJ2L\\Mynatt et al. - 1998 - Designing audio aura.pdf:application/pdf},
}

@article{gaver_sonicfinder_1989,
	title = {The {SonicFinder}: {An} {Interface} {That} {Uses} {Auditory} {Icons}},
	volume = {4},
	issn = {0737-0024},
	shorttitle = {The {SonicFinder}},
	url = {https://www.tandfonline.com/doi/abs/10.1207/s15327051hci0401_3},
	doi = {10.1207/s15327051hci0401_3},
	abstract = {The appropriate use of nonspeech sounds has the potential to add a great deal to the functionality of computer interfaces. Sound is a largely unexploited medium of output, even though it plays an integral role in our everyday encounters with the world, a role that is complementary to vision. Sound should be used in computers as it is in the world, where it conveys information about the nature of sound-producing events. Such a strategy leads to auditory icons, which are everyday sounds meant to convey information about computer events by analogy with everyday events. Auditory icons are an intuitively accessible way to use sound to provide multidimensional, organized information to users. These ideas are instantiated in the SonicFinder, which is an auditory interface I developed at Apple Computer, Inc. In this interface, information is conveyed using auditory icons as well as standard graphical feedback. I discuss how events are mapped to auditory icons in the SonicFinder, and illustrate how sound is used by describing a typical interaction with this interface. Two major gains are associated with using sound in this interface: an increase in direct engagement with the model world of the computer and an added flexibility for users in getting information about that world. These advantages seem to be due to the iconic nature of the mappings used between sound and the information it is to convey. I discuss sound effects and source metaphors as methods of extending auditory icons beyond the limitations implied by literal mappings, and I speculate on future directions for such interfaces.},
	number = {1},
	urldate = {2023-03-13},
	journal = {Human–Computer Interaction},
	author = {Gaver, William W.},
	month = mar,
	year = {1989},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1207/s15327051hci0401\_3},
	pages = {67--94},
	file = {Full Text PDF:C\:\\Users\\tech\\Zotero\\storage\\WQ49NPZE\\Gaver - 1989 - The SonicFinder An Interface That Uses Auditory I.pdf:application/pdf},
}

@inproceedings{bederson_audio_1995,
	address = {Denver, Colorado, United States},
	title = {Audio augmented reality: a prototype automated tour guide},
	isbn = {978-0-89791-755-1},
	shorttitle = {Audio augmented reality},
	url = {http://portal.acm.org/citation.cfm?doid=223355.223526},
	doi = {10.1145/223355.223526},
	abstract = {Augmented reality (or computer augmented environments as it is sometimes called) uses computers to enhance the richness of the real world. It differs from virtual reality in that it doesn’t attempt to replace the real world. Our prototype automated tour guide superimposes audio on the world based on where a user is located. We propose this technique for use as an automated tour guide in museums and expect it will enhance the social aspects of museum visits, compared to taped tour guides.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {Conference companion on {Human} factors in computing systems  - {CHI} '95},
	publisher = {ACM Press},
	author = {Bederson, Benjamin B.},
	year = {1995},
	pages = {210--211},
	file = {Bederson - 1995 - Audio augmented reality a prototype automated tou.pdf:C\:\\Users\\tech\\Zotero\\storage\\VRUVD9NX\\Bederson - 1995 - Audio augmented reality a prototype automated tou.pdf:application/pdf},
}

@article{blattner_earcons_1989,
	title = {Earcons and {Icons}: {Their} {Structure} and {Common} {Design} {Principles}},
	volume = {4},
	issn = {0737-0024},
	shorttitle = {Earcons and {Icons}},
	url = {https://www.tandfonline.com/doi/abs/10.1207/s15327051hci0401_1},
	doi = {10.1207/s15327051hci0401_1},
	abstract = {In this article we examine earcons, which are audio messages used in the user-computer interface to provide information and feedback to the user about computer entities. (Earcons include messages and functions, as well as states and labels.) We identify some design principles that are common to both visual symbols and auditory messages, and discuss the use of representational and abstract icons and earcons. We give some examples of audio patterns that may be used to design modules for earcons, which then may be assembled into larger groupings called families. The modules are single pitches or rhythmicized sequences of pitches called motives. The families are constructed about related motives that serve to identify a family of related messages. Issues concerned with learning and remembering earcons are discussed.},
	number = {1},
	urldate = {2023-03-13},
	journal = {Human–Computer Interaction},
	author = {Blattner, Meera M. and Sumikawa, Denise A. and Greenberg, Robert M.},
	month = mar,
	year = {1989},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1207/s15327051hci0401\_1},
	pages = {11--44},
	file = {Full Text PDF:C\:\\Users\\tech\\Zotero\\storage\\46YRM6CY\\Blattner et al. - 1989 - Earcons and Icons Their Structure and Common Desi.pdf:application/pdf},
}

@misc{accuweather,
author= {AccuWeather},
title= {{AccuWeather} {(Android)}},
note={v8.9.0-29-google},
url={https://www.accuweather.com/},
year={2023},
howpublished={Mobile Software}
}

@misc{roettgers_another_2020,
	title = {Another company is giving up on {AR}. {This} time, it’s {Bose}. - {Protocol}},
	url = {https://www.protocol.com/bose-gives-up-on-augmented-reality},
	abstract = {The audio company says its ambitious AR initiative "didn't become what we envisioned."},
	language = {en},
	urldate = {2023-03-18},
	author = {Roettgers, Janko},
	month = jun,
	year = {2020},
	file = {Snapshot:C\:\\Users\\tech\\Zotero\\storage\\3N9LRA33\\bose-gives-up-on-augmented-reality.html:text/html},
}


@inproceedings{kravchenko_ranking_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Networks} and {Systems}},
	title = {Ranking {Requirements} {Using} {MoSCoW} {Methodology} in {Practice}},
	isbn = {978-3-031-09073-8},
	doi = {10.1007/978-3-031-09073-8_18},
	abstract = {Requirement prioritization is performed in order to analyze business requirements and to define the required capabilities leading to potential solutions that will fulfill stakeholder needs. During the analysis, the needs and informal concerns are transformed into formal solution requirements describing the behavior of solution components in details. The developed models can describe the current state of the organization and are used for validating the solution scope among mangers and stakeholders. This facilitates identification of open opportunities for improvement or assists stakeholders in understanding the current state. Several techniques have been applied for requirements prioritization in a case study of the conventional commercial bank where the main problems of the communication management process have been formulated and illustrated by the fishbone diagram. The MoSCoW technique has been applied to identification of four requirement groups, whose impact on the results principally differ within the scope of the identified problems. The obtained list of prioritized requirements should be used on next project stages since it will be exploited by the managers during their planning future jobs on the solution implementation. The paper results are aimed at helping the stakeholders develop a common point of view on the strategic goals of the project.},
	language = {en},
	booktitle = {Cybernetics {Perspectives} in {Systems}},
	publisher = {Springer International Publishing},
	author = {Kravchenko, Tatiana and Bogdanova, Tatiana and Shevgunov, Timofey},
	editor = {Silhavy, Radek},
	year = {2022},
	keywords = {Fishbone diagram, MoSCoW technique, Requirement prioritization, Requirements},
	pages = {188--199},
	file = {Full Text PDF:C\:\\Users\\tech\\Zotero\\storage\\UXL939ZS\\Kravchenko et al. - 2022 - Ranking Requirements Using MoSCoW Methodology in P.pdf:application/pdf},
}

@incollection{puiatti_whats_2012,
	address = {Berlin, Heidelberg},
	title = {What’s around {Me}? {Spatialized} {Audio} {Augmented} {Reality} for {Blind} {Users} with a {Smartphone}},
	volume = {104},
	isbn = {978-3-642-30972-4 978-3-642-30973-1},
	shorttitle = {What’s around {Me}?},
	url = {http://link.springer.com/10.1007/978-3-642-30973-1_5},
	abstract = {Numerous projects have investigated assistive navigation technologies for the blind community, tackling challenges ranging from interface design to sensory substitution. However, none of these have successfully integrated what we consider to be the three factors necessary for a widely deployable system that delivers a rich experience of one’s environment: implementation on a commodity device, use of a preexisting worldwide point of interest (POI) database, and a means of rendering the environment that is superior to a naive playback of spoken text. Our “In Situ Audio Services” (ISAS) application responds to these needs, allowing users to explore an urban area without necessarily having a particular destination in mind. We describe the technical aspects of its implementation, user requirements, interface design, safety concerns, POI data source issues, and further requirements to make the system practical on a wider basis. Initial qualitative feedback from blind users is also discussed.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {Mobile and {Ubiquitous} {Systems}: {Computing}, {Networking}, and {Services}},
	publisher = {Springer Berlin Heidelberg},
	author = {Blum, Jeffrey R. and Bouchard, Mathieu and Cooperstock, Jeremy R.},
	editor = {Puiatti, Alessandro and Gu, Tao},
	year = {2012},
	doi = {10.1007/978-3-642-30973-1_5},
	note = {Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering},
	pages = {49--62},
	file = {Blum et al. - 2012 - What’s around Me Spatialized Audio Augmented Real.pdf:C\:\\Users\\tech\\Zotero\\storage\\EQXKB5XR\\Blum et al. - 2012 - What’s around Me Spatialized Audio Augmented Real.pdf:application/pdf},
}


@article{klatzky_cognitive_2006,
	title = {Cognitive load of navigating without vision when guided by virtual sound versus spatial language.},
	volume = {12},
	issn = {1939-2192, 1076-898X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1076-898X.12.4.223},
	doi = {10.1037/1076-898X.12.4.223},
	abstract = {A vibrotactile N-back task was used to generate cognitive load while participants were guided along virtual paths without vision. As participants stepped in place, they moved along a virtual path of linear segments. Information was provided en route about the direction of the next turning point, by spatial language (“left,” “right,” or “straight”) or virtual sound (i.e., the perceived azimuth of the sound indicated the target direction). The authors hypothesized that virtual sound, being processed at direct perceptual levels, would have lower load than even simple language commands, which require cognitive mediation. As predicted, whereas the guidance modes did not differ significantly in the no-load condition, participants showed shorter distance traveled and less time to complete a path when performing the N-back task while navigating with virtual sound as guidance. Virtual sound also produced better N-back performance than spatial language. By indicating the superiority of virtual sound for guidance when cognitive load is present, as is characteristic of everyday navigation, these results have implications for guidance systems for the visually impaired and others.},
	language = {en},
	number = {4},
	urldate = {2023-03-27},
	journal = {Journal of Experimental Psychology: Applied},
	author = {Klatzky, Roberta L. and Marston, James R. and Giudice, Nicholas A. and Golledge, Reginald G. and Loomis, Jack M.},
	month = dec,
	year = {2006},
	pages = {223--232},
	file = {Klatzky et al. - 2006 - Cognitive load of navigating without vision when g.pdf:C\:\\Users\\tech\\Zotero\\storage\\IF294MPD\\Klatzky et al. - 2006 - Cognitive load of navigating without vision when g.pdf:application/pdf},
}

@article{brewster_multimodal_2003,
	title = {Multimodal ‘{Eyes}-{Free}’ {Interaction} {Techniques} for {Wearable} {Devices}},
	abstract = {Mobile and wearable computers present input/output problems due to limited screen space and interaction techniques. When mobile, users typically focus their visual attention on navigating their environment - making visually demanding interface designs hard to operate. This paper presents two multimodal interaction techniques designed to overcome these problems and allow truly mobile, ‘eyes-free’ device use. The first is a 3D audio radial pie menu that uses head gestures for selecting items. An evaluation of a range of different audio designs showed that egocentric sounds reduced task completion time, perceived annoyance, and allowed users to walk closer to their preferred walking speed. The second is a sonically enhanced 2D gesture recognition system for use on a belt-mounted PDA. An evaluation of the system with and without audio feedback showed users’ gestures were more accurate when dynamically guided by audio-feedback. These novel interaction techniques demonstrate effective alternatives to visual-centric interface designs on mobile devices.},
	language = {en},
	number = {5},
	journal = {NEW HORIZONS},
	author = {Brewster, Stephen and Lumsden, Joanna and Bell, Marek and Hall, Malcolm and Tasker, Stuart},
	year = {2003},
	pages = {8},
	file = {Brewster et al. - 2003 - Multimodal ‘Eyes-Free’ Interaction Techniques for .pdf:C\:\\Users\\tech\\Zotero\\storage\\NFVWHZNX\\Brewster et al. - 2003 - Multimodal ‘Eyes-Free’ Interaction Techniques for .pdf:application/pdf},
}

@misc{corporation_bose_2018,
	title = {Bose {Global} {Press} {Room} - {Bose} {Introduces} {Audio} {Augumented} {Reality} {Platform}},
	url = {https://www.bose.com/en_us/pressroom/archive/2018/bose-introduces-audio-augumented-reality-platform.html},
	language = {en-US},
	urldate = {2023-03-28},
	author = {{Bose Corporation}},
	month = mar,
	year = {2018},
    howpublished = {Online},
    note = {{Press Release}},
	file = {Snapshot:C\:\\Users\\tech\\Zotero\\storage\\URWMKXY5\\bose-introduces-audio-augumented-reality-platform.html:text/html},
}

@misc{takahashi_bose_2019,
	title = {Bose {AR} shows off augmented reality audio games},
	url = {https://venturebeat.com/business/bose-ar-shows-off-augmented-reality-audio-games/},
	abstract = {Bose AR is augmented reality glasses with spatial audio. It makes possible games and apps that depend on your ability to hear from different directions.},
	language = {en-US},
	urldate = {2023-03-29},
	journal = {VentureBeat},
	author = {Takahashi, Dean},
	month = apr,
	year = {2019},
	file = {Snapshot:C\:\\Users\\tech\\Zotero\\storage\\K9IPCFFY\\bose-ar-shows-off-augmented-reality-audio-games.html:text/html},
}

@inproceedings{brewster_evaluation_1993,
	address = {Amsterdam, The Netherlands},
	title = {An evaluation of earcons for use in auditory human-computer interfaces},
	isbn = {978-0-89791-575-5},
	url = {http://portal.acm.org/citation.cfm?doid=169059.169179},
	doi = {10.1145/169059.169179},
	language = {en},
	urldate = {2023-03-13},
	booktitle = {Proceedings of the {SIGCHI} conference on {Human} factors in computing systems  - {CHI} '93},
	publisher = {ACM Press},
	author = {Brewster, Stephen A. and Wright, Peter C. and Edwards, Alistair D. N.},
	year = {1993},
	pages = {222--227},
	file = {Brewster et al. - 1993 - An evaluation of earcons for use in auditory human.pdf:C\:\\Users\\tech\\Zotero\\storage\\HLV5RQ9L\\Brewster et al. - 1993 - An evaluation of earcons for use in auditory human.pdf:application/pdf},
}

@inproceedings{mcgookin_pulse_2011,
	address = {Stockholm, Sweden},
	title = {{PULSE}: an auditory display to provide a social vibe},
	isbn = {978-1-4503-0883-0},
	shorttitle = {{PULSE}},
	url = {http://dl.acm.org/citation.cfm?doid=2019335.2019338},
	doi = {10.1145/2019335.2019338},
	abstract = {An increasing amount of social media is being tagged with the location of its creation. However, little investigation of how these tagged media can be used has been undertaken. We seek to exploit their auditory presentation in a system called PULSE. PULSE attempts to provide an understanding of the people, places and activities that are happening in the user’s current locale. We outline the design of PULSE and how both message and meta-data can be implicitly and explicitly incorporated into an auditory display. We outline our plans for future evaluations to further consider how social geo-data can be aurally presented to users.},
	language = {en},
	urldate = {2022-10-04},
	booktitle = {Proceedings of {Interacting} with {Sound} {Workshop} on {Exploring} {Context}-{Aware}, {Local} and {Social} {Audio} {Applications} - {IwS} '11},
	publisher = {ACM Press},
	author = {McGookin, David and Brewster, Stephen},
	year = {2011},
	pages = {12--15},
	file = {McGookin and Brewster - 2011 - PULSE an auditory display to provide a social vib.pdf:C\:\\Users\\tech\\Zotero\\storage\\7HEYAYTT\\McGookin and Brewster - 2011 - PULSE an auditory display to provide a social vib.pdf:application/pdf},
}
